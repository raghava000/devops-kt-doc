<!DOCTYPE html>
<html lang="en">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>DevOps Knowledge Transfer: Weeks 1-3</title>
    <style>
        body {
            font-family: -apple-system, BlinkMacSystemFont, "Segoe UI", Roboto, Helvetica, Arial, sans-serif;
            background-color: #f8f9fa;
            color: #212529;
            margin: 0;
            padding: 2em;
            line-height: 1.6;
        }
        .container { max-width: 1000px; margin: 0 auto; }
        .main-title { text-align: center; color: #2c3e50; margin-bottom: 2em; }
        .week-section { margin-bottom: 2.5em; }
        .week-title {
            font-size: 2em;
            color: #fff;
            padding: 0.5em 1em;
            border-radius: 8px;
            margin-bottom: 1em;
        }
        .week-1-title { background-color: #3498db; }
        .week-2-title { background-color: #e74c3c; }
        .week-3-title { background-color: #9b59b6; }
        details {
            background: #fff;
            border-radius: 8px;
            margin-bottom: 1em;
            box-shadow: 0 2px 8px rgba(0,0,0,0.07);
            border-left: 5px solid #bdc3c7;
            transition: all 0.3s ease;
        }
        details[open] { border-left-color: inherit; }
        .week-1 details[open] { border-left-color: #3498db; }
        .week-2 details[open] { border-left-color: #e74c3c; }
        .week-3 details[open] { border-left-color: #9b59b6; }
        summary {
            padding: 1em 1.5em;
            cursor: pointer;
            font-size: 1.2em;
            font-weight: 600;
            color: #34495e;
            list-style: none;
            outline: none;
        }
        summary::-webkit-details-marker { display: none; }
        summary::before {
            content: 'â–º';
            margin-right: 0.8em;
            font-size: 0.8em;
            transition: transform 0.2s;
            display: inline-block;
        }
        details[open] > summary::before { transform: rotate(90deg); }
        .day-content { padding: 0 1.5em 1.5em 1.5em; border-top: 1px solid #ecf0f1; }
        h4 {
            font-size: 1.1em;
            color: #2980b9;
            border-bottom: 2px solid #ecf0f1;
            padding-bottom: 0.3em;
            margin-top: 1.5em;
        }
        .week-2 h4 { color: #c0392b; }
        .week-3 h4 { color: #8e44ad; }
        p { color: #555; }
        code {
            background-color: #ecf0f1;
            padding: 0.2em 0.4em;
            border-radius: 4px;
            font-family: "SFMono-Regular", Consolas, "Liberation Mono", Menlo, Courier, monospace;
            font-size: 0.9em;
        }
        pre {
            background-color: #2c3e50;
            color: #ecf0f1;
            padding: 1em;
            border-radius: 6px;
            overflow-x: auto;
            white-space: pre-wrap;
            font-size: 0.9em;
        }
        pre code { background: none; padding: 0; }
        .challenge-section {
            background-color: #fff5f5;
            border-left: 4px solid #c0392b;
            padding: 1em;
            margin: 1.5em 0;
        }
        .challenge-section strong { color: #c0392b; }
    </style>
</head>
<body>
    <div class="container">
        <h1 class="main-title">DevOps Knowledge Transfer (Weeks 1-3)</h1>

        <!-- WEEK 1 SECTION -->
        <section class="week-section">
            <h2 class="week-title week-1-title">Week 1: Container Orchestration with Docker & Kubernetes</h2>
            <details>
                <summary>Day 1-2: Code to Container to Cloud Registry</summary>
                <div class="day-content">
                    <h4>Goal:</h4>
                    <p>Package a Python app into a portable Docker Image, optimize it with a <code>.dockerignore</code>, and push it to a private Artifact Registry.</p>
                    <h4>The "Why" (Technical Concept):</h4>
                    <p>Containerization decouples an application from its environment, ensuring it runs consistently. A private registry is a secure, centralized location to store and version these application packages (Images).</p>
                    <h4>The "How" (Commands & Files):</h4>
<pre><code># Dockerfile - The application blueprint
FROM python:3.9-slim
WORKDIR /app
COPY requirements.txt .
RUN pip install -r requirements.txt
COPY . .
CMD ["python", "app.py"]

# .dockerignore - Files to exclude from the build
__pycache__/
*.pyc

# Build the image from the Dockerfile in the current directory
docker build -t my-app .

# Run the container, mapping host port 5001 to container port 5000
docker run -p 5001:5000 my-app

# Tag the local image with its full remote address
docker tag my-app asia-south1-docker.pkg.dev/PROJECT_ID/REPO_NAME/my-app:v1.0.0

# Push the tagged image to the remote registry
docker push asia-south1-docker.pkg.dev/PROJECT_ID/REPO_NAME/my-app:v1.0.0
</code></pre>
                </div>
            </details>
            <details>
                <summary>Day 3-7: Deploying a Complete Application on GKE</summary>
                <div class="day-content">
                    <h4>Goal:</h4>
                    <p>Deploy a multi-replica, self-healing, auto-scaling, and updatable application on a managed GKE cluster.</p>
                    <h4>The "Why" (Technical Concept):</h4>
                    <p>Orchestration automates the management of containerized applications. You define a desired state using declarative YAML files, and Kubernetes works to make reality match that declaration.</p>
                    <h4>The "How" (Commands & Files):</h4>
<pre><code># --- Key Commands ---
# Create a new GKE Autopilot cluster (in Cloud Shell)
gcloud container clusters create my-cluster --zone=us-central1-c --enable-autopilot

# Configure kubectl to connect to the new cluster
gcloud container clusters get-credentials my-cluster --zone=us-central1-c

# Apply a YAML file to the cluster
kubectl apply -f my-deployment.yaml

# --- Key Debugging Commands ---
# kubectl get pods -n <namespace>      (Check high-level status)
# kubectl describe pod <pod-name> -n <namespace> (Check Events for errors)
# kubectl logs <pod-name> -n <namespace> (See application's direct output)

# --- deployment.yaml (Example) ---
apiVersion: apps/v1
kind: Deployment
metadata:
  name: my-web-app
spec:
  replicas: 3
  selector:
    matchLabels:
      app: web
  template:
    metadata:
      labels:
        app: web
    spec:
      containers:
      - name: nginx
        image: nginx:1.14.2
        ports:
        - containerPort: 80
        # CRITICAL for Autopilot: Resource requests must be set.
        resources:
          requests:
            memory: "64Mi"
            cpu: "250m"

# --- service.yaml (Example) ---
apiVersion: v1
kind: Service
metadata:
  name: my-web-service
spec:
  type: ClusterIP
  selector:
    app: web
  ports:
  - protocol: TCP
    port: 80
    targetPort: 80
</code></pre>
                </div>
            </details>
            <div class="challenge-section">
                <h4>Week 1 Biggest Challenge: "The Cloud Fortress"</h4>
                <p><strong>The Problem:</strong> My container worked locally but was unreachable on a cloud VM, resulting in a cascade of errors: <code>Connection Refused</code>, <code>Permission Denied</code>, <code>Unauthorized</code>.</p>
                <p><strong>The Debugging Process:</strong> I learned that a cloud environment is secure by default. I had to solve problems at three distinct layers:</p>
                <ol>
                    <li><strong>Networking:</strong> Realized I must use the VM's <strong>External IP</strong>, not <code>localhost</code>.</li>
                    <li><strong>Firewall:</strong> Discovered that ports are blocked by default and learned to create a <strong>firewall rule</strong> (<code>gcloud compute firewall-rules create</code>) to open port 5001.</li>
                    <li><strong>IAM:</strong> Understood that VMs have a separate identity (Service Account). To fix the final `push` failure, I had to grant this identity the <strong>`Artifact Registry Writer` role</strong> (<code>gcloud projects add-iam-policy-binding</code>).</li>
                </ol>
                <p><strong>What I Learned:</strong> Cloud deployment requires a holistic understanding of networking, security, and identity. You must explicitly grant access at every layer.
                </p>
            </div>
        </section>

        <!-- WEEK 2 SECTION -->
        <section class="week-section">
            <h2 class="week-title week-2-title">Week 2: Configuration Management with Ansible</h2>
            <details>
                <summary>Day 8-11: Multi-Tier Application Setup</summary>
                <div class="day-content">
                    <h4>Goal:</h4>
                    <p>Use Ansible to automate the setup of a two-tier application (web + database) on separate VMs, including SSH setup and network configuration.</p>
                    <h4>The "Why" (Technical Concept):</h4>
                    <p>Ansible provides agentless, repeatable server configuration. It uses Playbooks to define a desired state. Idempotency ensures that running the same playbook multiple times only makes changes if necessary, guaranteeing a consistent state.</p>
                    <h4>The "How" (Commands & Files):</h4>
<pre><code># inventory.ini - The "address book" for your servers
[webservers]
node1 ansible_host=IP_ADDRESS ansible_user=USERNAME ansible_ssh_private_key_file=~/.ssh/google_compute_engine

# ansible.cfg - Configuration for Ansible itself
[defaults]
inventory = inventory.ini
host_key_checking = false

# playbook.yaml - A to-do list for a server
---
- name: Install and run a container
  hosts: webservers
  become: yes # Use sudo
  tasks:
    - name: Install Docker
      apt:
        name: docker-ce
        state: present
    - name: Run a container
      docker_container:
        name: my-container
        image: nginx
        state: started

# Run a playbook
ansible-playbook playbook.yaml
</code></pre>
                </div>
            </details>
            <details>
                <summary>Day 12-14: Professionalizing Ansible</summary>
                <div class="day-content">
                    <h4>Goal:</h4>
                    <p>Organize the playbooks into reusable Roles, secure the database password with Vault, and implement a zero-downtime rolling update strategy.</p>
                    <h4>The "Why" (Technical Concept):</h4>
                    <p>Storing plain-text secrets in Git is a critical security risk; Ansible Vault encrypts this data. Roles provide a standardized directory structure for building clean, reusable automation code. Rolling updates (`serial: 1`) are essential for deploying changes to a fleet of servers without an outage.</p>
                    <h4>The "How" (Commands & Files):</h4>
<pre><code># Create a standard role structure
ansible-galaxy init roles/my-role

# site.yaml - The master plan that calls the roles
---
- name: Configure Web Server
  hosts: webservers
  roles:
    - web

# Create an encrypted file for secrets
ansible-vault create roles/db/vars/main.yaml

# Edit an existing encrypted file
ansible-vault edit roles/db/vars/main.yaml

# Run a playbook that uses a vault, prompting for the password
ansible-playbook site.yaml --ask-vault-pass
</code></pre>
                </div>
            </details>
            <div class="challenge-section">
                <h4>Week 2 Biggest Challenge: "The Order of Operations"</h4>
                <p><strong>The Problem:</strong> My Ansible playbooks kept failing with subtle errors like <code>Group docker does not exist</code>, <code>package not found</code>, or <code>Permission denied</code> when trying to run Docker commands.</p>
                <p><strong>The Debugging Process:</strong> I learned that automation scripts must be perfectly ordered and self-contained.</p>
                <ol>
                    <li><strong>Dependency Order:</strong> I diagnosed that the `docker` group is only created *after* Docker is installed. My task order was wrong. The fix was to ensure the "Install Docker" tasks ran *before* the "Add user to group" task.</li>
                    <li><strong>Self-Contained Roles:</strong> I realized my `db` role failed because it didn't have the Docker installation tasks at all. The fix was to make each role complete, containing all steps needed to build its service from a blank server.</li>
                </ol>
                <p><strong>What I Learned:</strong> Professional automation code must be idempotent, self-contained, and have a logically correct order of operations.
                </p>
            </div>
        </section>

        <!-- WEEK 3 SECTION -->
        <section class="week-section">
            <h2 class="week-title week-3-title">Week 3: CI/CD Pipelines with Jenkins & GitHub Actions</h2>
            <details>
                <summary>Day 15-21: Building an End-to-End Pipeline</summary>
                <div class="day-content">
                    <h4>Goal:</h4>
                    <p>Build a full CI/CD pipeline that automatically builds a Docker image on a code push, pushes it to a private registry, and deploys it to a GKE cluster. Then, replicate this with a modern, serverless alternative.</p>
                    <h4>The "Why" (Technical Concept):</h4>
                    <p>A CI/CD pipeline automates the "code-to-customer" workflow. Defining this process as code (`Jenkinsfile` or GitHub Actions YAML) makes it version-controlled and repeatable. Separating environments (`dev`/`prod`) and having automated rollbacks are mandatory safety practices.</p>
                    <h4>The "How" (Commands & Files):</h4>
<pre><code># Jenkinsfile: The final, robust pipeline with rollback
pipeline {
    agent any
    // ... environment variables ...
    stages {
        stage('Build and Push') { /* ... */ }
        stage('Deploy and Verify') {
            steps {
                script {
                    // Logic to set TARGET_NAMESPACE based on env.BRANCH_NAME
                    try {
                        // 1. Deploy the new, potentially broken version
                        sh "kubectl set image ..."
                        // 2. Verify health (this command fails on a bad rollout)
                        sh "kubectl rollout status ..."
                    } catch (any) {
                        // 3. If verification fails, automatically roll back
                        sh "kubectl rollout undo ..."
                        error "Deployment failed and was rolled back."
                    }
                }
            }
        }
    }
}

# .github/workflows/main.yaml - The serverless alternative
name: Build and Deploy to GKE
on:
  push:
    branches: [ "main" ]
permissions:
  contents: 'read'
  id-token: 'write'
jobs:
  build-and-deploy:
    runs-on: ubuntu-latest
    steps:
      - name: Checkout
        uses: actions/checkout@v4
      - name: Authenticate to Google Cloud
        uses: 'google-github-actions/auth@v2'
        with:
          credentials_json: '${{ secrets.GCP_CREDENTIALS }}'
      # ... subsequent build, push, and deploy steps
</code></pre>
                </div>
            </details>
            <div class="challenge-section">
                <h4>Week 3 Biggest Challenge: "The Broken Factory"</h4>
                <p><strong>The Problem:</strong> I was plagued by a relentless series of cryptic Jenkins errors (`NoSuchMethodError`, stuck plugins, `Invalid control character`, `credential type mismatch`) even after correctly following instructions.</p>
                <p><strong>The Debugging Process:</strong> This was a masterclass in resilience. I learned that sometimes the tool itself is the problem.</p>
                <ol>
                    <li><strong>Plugin Hell:</strong> I proved the `withGoogleServiceAccount` step was fundamentally broken in my environment, likely due to a corrupted Jenkins state or plugin conflicts.</li>
                    <li><strong>The Professional Fix:</strong> I stopped trying to fix the "magic." The solution was to fall back to a more robust, fundamental pattern: use a basic `Secret text` credential, write it to a temporary file with `writeFile` (to avoid shell corruption from `echo`), explicitly authenticate with `gcloud auth`, do the work, and then securely clean up with `rm`.</li>
                    <li><strong>Serverless Alternative:</strong> I then rebuilt the entire pipeline in GitHub Actions, learning the modern, key-based authentication flow (`credentials_json`) and debugging a silent failure caused by a single missing character in a YAML file.</li>
                </ol>
                <p><strong>What I Learned:</strong> The most important skill is knowing when a tool is broken and having the fundamental knowledge to bypass its failures with a more direct, reliable solution.
                </p>
            </div>
        </section>
    </div>
</body>
</html>

